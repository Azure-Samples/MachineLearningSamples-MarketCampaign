{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review Sentiment Operationalization\n",
    "\n",
    "### Schema Generatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This script generates the scoring and schema files\n",
    "# necessary to operationalize the Market Campaign prediction sample\n",
    "# Init and run functions\n",
    "\n",
    "from azureml.api.schema.dataTypes import DataTypes\n",
    "from azureml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azureml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare the web service definition by authoring\n",
    "# init() and run() functions. Test the fucntions\n",
    "# before deploying the web service.\n",
    "\n",
    "def init():\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    # load the model file\n",
    "    global model\n",
    "    model = joblib.load('model_30.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "def run(input_df):\n",
    "    import json\n",
    "    \n",
    "    input_df.columns = ['input_column'] \n",
    "    \n",
    "    stop_words_df = pd.read_csv('StopWords.csv')\n",
    "    stop_words = set(stop_words_df[\"Col1\"].tolist())\n",
    "    for item in string.ascii_lowercase: #load stop words\n",
    "        if item != \"i\":\n",
    "            stop_words.add(item)\n",
    "\n",
    "    input_column = []\n",
    "    for line in input_df.input_column:\n",
    "        value = \" \".join(item.lower()\n",
    "                         for item in RegexpTokenizer(r'\\w+').tokenize(line)\n",
    "                         if item.lower() not in stop_words)\n",
    "        input_column.append(value)\n",
    "    input_df.input_column = input_column\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    input_list = input_df[\"input_column\"].tolist()\n",
    "\n",
    "    # Tokenize the sentences in text_list and remove morphological affixes from words.\n",
    "\n",
    "    def stem_tokens(tokens, stemmer_model):\n",
    "        '''\n",
    "        :param tokens: tokenized word list\n",
    "        :param stemmer: remove stemmer\n",
    "        :return:  tokenized and stemmed words\n",
    "        '''\n",
    "        return [stemmer_model.stem(original_word) for original_word in tokens]\n",
    "\n",
    "    def tokenize(text):\n",
    "        '''\n",
    "        :param text: raw test\n",
    "        :return: tokenized and stemmed words\n",
    "        '''\n",
    "        tokens = text.strip().split(\" \")\n",
    "        return stem_tokens(tokens, stemmer)\n",
    "\n",
    "    # Initialize the TfidfVectorizer to compute tf-idf for each word\n",
    "\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', max_df=160000,\n",
    "                            min_df=1, norm=\"l2\", use_idf=True)\n",
    "    tfs = tfidf.fit_transform(input_list)\n",
    "    \n",
    "    pred = model.predict(tfs[0, :30])\n",
    "    return json.dumps(str(pred[0]))\n",
    "    #return pred[0]\n",
    "print('executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I absolutely love my bank. There's a reason th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  I absolutely love my bank. There's a reason th..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(data=[[\"I absolutely love my bank. There's a reason this bank's customer base is so strong--their customer service actually acts like people and not robots. I love that anytime my card is swiped, I'm instantly notified. And the built in budgeting app is something that really makes life easier. The biggest setback is not being able to deposit cash (you have to get a money order), and if you have another, non-simple bank account, transferring money between accounts can take a few days, which frankly isn't acceptable with most ACH taking a business day or less. Overall, it's a great bank, and I would recommend it to anyone.\"]], columns=['review'])\n",
    "df1.dtypes\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsvmadmin\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\dsvmadmin\\AppData\\local\\AmlWorkbench\\Python\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I absolutely love my bank. There's a reason th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  I absolutely love my bank. There's a reason th..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init()\n",
    "input1 = pd.DataFrame(data=[[\"I absolutely love my bank. There's a reason this bank's customer base is so strong--their customer service actually acts like people and not robots. I love that anytime my card is swiped, I'm instantly notified. And the built in budgeting app is something that really makes life easier. The biggest setback is not being able to deposit cash (you have to get a money order), and if you have another, non-simple bank account, transferring money between accounts can take a few days, which frankly isn't acceptable with most ACH taking a business day or less. Overall, it's a great bank, and I would recommend it to anyone.\"]], columns=['review'])\n",
    "input1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"0\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run(input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'input_df': {'internal': 'gANjYXp1cmVtbC5hcGkuc2NoZW1hLnBhbmRhc1V0aWwKUGFuZGFzU2NoZW1hCnEAKYFxAX1xAihYCgAAAHNjaGVtYV9tYXBxA31xBFgGAAAAcmV2aWV3cQVjbnVtcHkKZHR5cGUKcQZYAgAAAE84cQdLAEsBh3EIUnEJKEsDWAEAAAB8cQpOTk5K/////0r/////Sz90cQtic1gMAAAAY29sdW1uX3R5cGVzcQxdcQ1oCWFYBQAAAHNoYXBlcQ5LAUsBhnEPWAwAAABjb2x1bW5fbmFtZXNxEF1xEWgFYXViLg==',\n",
       "   'swagger': {'example': [{'review': \"I absolutely love my bank. There's a reason this bank's customer base is so strong--their customer service actually acts like people and not robots. I love that anytime my card is swiped, I'm instantly notified. And the built in budgeting app is something that really makes life easier. The biggest setback is not being able to deposit cash (you have to get a money order), and if you have another, non-simple bank account, transferring money between accounts can take a few days, which frankly isn't acceptable with most ACH taking a business day or less. Overall, it's a great bank, and I would recommend it to anyone.\"}],\n",
       "    'items': {'properties': {'review': {'type': 'string'}}, 'type': 'object'},\n",
       "    'type': 'array'},\n",
       "   'type': 3}}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = {\"input_df\": SampleDefinition(DataTypes.PANDAS, df1)}\n",
    "\n",
    "# The prepare statement writes the scoring file (main.py) and\n",
    "# the schema file (senti_service_schema.json) the the output folder.\n",
    "\n",
    "generate_schema(run_func=run, inputs=inputs, filepath='senti_service_schema.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    import numpy\n",
    "    import scipy\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    global model\n",
    "    import pickle\n",
    "    f = open('./model_30.pkl', 'rb')\n",
    "    model = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executed\n"
     ]
    }
   ],
   "source": [
    "def run(inputString):\n",
    "    import json\n",
    "    import numpy\n",
    "    try:\n",
    "        input_list = json.loads(inputString)\n",
    "    except ValueError:\n",
    "        return \"bad input: expecting a JSON encoded list of lists.\"\n",
    "    input_df = pd.DataFrame(input_list, columns=['review'])\n",
    "    if (input_df.shape != (1, 1)):\n",
    "        return 'bad input: expecting a JSON encoded list of lists of shape (1,1).'\n",
    "    \n",
    "    input_df.columns = ['input_column'] \n",
    "    \n",
    "    stop_words_df = pd.read_csv('StopWords.csv')\n",
    "    stop_words = set(stop_words_df[\"Col1\"].tolist())\n",
    "    for item in string.ascii_lowercase: #load stop words\n",
    "        if item != \"i\":\n",
    "            stop_words.add(item)\n",
    "\n",
    "    input_column = []\n",
    "    for line in input_df.input_column:\n",
    "        value = \" \".join(item.lower()\n",
    "                         for item in RegexpTokenizer(r'\\w+').tokenize(line)\n",
    "                         if item.lower() not in stop_words)\n",
    "        input_column.append(value)\n",
    "    input_df.input_column = input_column\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    input_list = input_df[\"input_column\"].tolist()\n",
    "\n",
    "    # Tokenize the sentences in text_list and remove morphological affixes from words.\n",
    "\n",
    "    def stem_tokens(tokens, stemmer_model):\n",
    "        '''\n",
    "        :param tokens: tokenized word list\n",
    "        :param stemmer: remove stemmer\n",
    "        :return:  tokenized and stemmed words\n",
    "        '''\n",
    "        return [stemmer_model.stem(original_word) for original_word in tokens]\n",
    "\n",
    "    def tokenize(text):\n",
    "        '''\n",
    "        :param text: raw test\n",
    "        :return: tokenized and stemmed words\n",
    "        '''\n",
    "        tokens = text.strip().split(\" \")\n",
    "        return stem_tokens(tokens, stemmer)\n",
    "\n",
    "    # Initialize the TfidfVectorizer to compute tf-idf for each word\n",
    "\n",
    "    tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english', max_df=160000,\n",
    "                            min_df=1, norm=\"l2\", use_idf=True)\n",
    "    tfs = tfidf.fit_transform(input_list)\n",
    "    \n",
    "    pred = model.predict(tfs[0, :30])\n",
    "    return json.dumps(str(pred[0]))\n",
    "    #return pred[0]\n",
    "print('executed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"0\"\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import json\n",
    "    init()\n",
    "    print (run(json.dumps([[\"I absolutely love my bank. There's a reason this bank's customer base is so strong--their customer service actually acts like people and not robots. I love that anytime my card is swiped, I'm instantly notified. And the built in budgeting app is something that really makes life easier. The biggest setback is not being able to deposit cash (you have to get a money order), and if you have another, non-simple bank account, transferring money between accounts can take a few days, which frankly isn't acceptable with most ACH taking a business day or less. Overall, it's a great bank, and I would recommend it to anyone.\"]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
